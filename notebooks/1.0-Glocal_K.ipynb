{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Importing all needed packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl2tU6kL8Ot3",
        "outputId": "379fe638-9ba3-4513-c9d0-6bbd279e03e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.parameter import Parameter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# In the original work a manual seed was used for reproducibility\n",
        "torch.manual_seed(11)\n",
        "\n",
        "# Autochoice of a device where the model will be trained on \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "### `Creating a function for data loading`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "outputs": [],
      "source": [
        "def load_data_100k(path='./', delimiter='\\t'):\n",
        "    \"\"\"\n",
        "        Function for loadign a data from the dataset, its formating, \n",
        "        and constracting a representation on which the model will train\n",
        "    \"\"\"\n",
        "\n",
        "    # Loading the data\n",
        "    train_data = np.loadtxt(path+'u3.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test_data = np.loadtxt(path+'u3.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total_data = np.concatenate((train_data, test_data), axis=0)\n",
        "\n",
        "    n_users = np.unique(total_data[:, 0]).size      # num of users\n",
        "    n_movies = np.unique(total_data[:, 1]).size     # num of movies\n",
        "    n_train_ratinigs = train_data.shape[0]          # num of training ratings\n",
        "    n_test_ratings = test_data.shape[0]             # num of test ratings\n",
        "\n",
        "    # Preparing a matrix where the data about the users preferences will be stored \n",
        "    train_ratings = np.zeros((n_movies, n_users), dtype='float32')\n",
        "    test_ratings = np.zeros((n_movies, n_users), dtype='float32')\n",
        "\n",
        "    # Filling the matrix\n",
        "    for i in range(n_train_ratinigs):\n",
        "        train_ratings[train_data[i, 1] - 1, train_data[i, 0] - 1] = train_data[i, 2]\n",
        "\n",
        "    for i in range(n_test_ratings):\n",
        "        test_ratings[test_data[i, 1] - 1, test_data[i, 0] - 1] = test_data[i, 2]\n",
        "\n",
        "    # Creating masks for loss calculations \n",
        "    train_mask = np.greater(train_ratings, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_mask = np.greater(test_ratings, 1e-12).astype('float32')\n",
        "\n",
        "    # Prinint some statistics\n",
        "    print('Data was successfully loaded:')\n",
        "    print('Num of users: {}'.format(n_users))\n",
        "    print('Num of movies: {}'.format(n_movies))\n",
        "    print('Num of training ratings: {}'.format(n_train_ratinigs))\n",
        "    print('Num of test ratings: {}'.format(n_test_ratings))\n",
        "\n",
        "    return n_movies, n_users, train_ratings, train_mask, test_ratings, test_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "### `Loading data on the specified path`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJqSSY33mgkw",
        "outputId": "54f7ca43-b9f7-4edb-8628-783a4513f4f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data was successfully loaded:\n",
            "Num of users: 943\n",
            "Num of movies: 1682\n",
            "Num of training ratings: 80000\n",
            "Num of test ratings: 20000\n"
          ]
        }
      ],
      "source": [
        "data_path = \"../data/raw/\"\n",
        "\n",
        "# Data Load\n",
        "try:\n",
        "    path = data_path + 'ml-100k/'\n",
        "    n_movies, n_users, train_ratings, train_mask, test_ratings, test_mask = load_data_100k(path=path, delimiter='\\t')\n",
        "except FileNotFoundError:\n",
        "    print('Error: Unable to load data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "### `Creating submodules of the final model and function for computing similarity between two sets of vectors U and V`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p1P6fgYiy28F"
      },
      "outputs": [],
      "source": [
        "def local_kernel(u, v):\n",
        "    \"\"\"\n",
        "        Function for computing a local kernel of local kernelised weight matrix\n",
        "    \"\"\"\n",
        "    dist = torch.norm(u - v, p=2, dim=2)\n",
        "    hat = torch.clamp(1. - dist**2, min=0.)\n",
        "    return hat\n",
        "\n",
        "# Class of a layer of the final model that will make one pass trough the one FC with some regularizations(sparsity and l2 ones)\n",
        "class KernelLayer(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hidden, n_dim, lambda_s, lambda_2, activation=nn.Sigmoid()):\n",
        "      super().__init__()\n",
        "      self.W = nn.Parameter(torch.randn(n_inputs, n_hidden))\n",
        "      self.u = nn.Parameter(torch.randn(n_inputs, 1, n_dim))\n",
        "      self.v = nn.Parameter(torch.randn(1, n_hidden, n_dim))\n",
        "      self.b = nn.Parameter(torch.randn(n_hidden))\n",
        "\n",
        "      self.lambda_s = lambda_s\n",
        "      self.lambda_2 = lambda_2\n",
        "\n",
        "      nn.init.xavier_uniform_(self.W, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.u, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.xavier_uniform_(self.v, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      nn.init.zeros_(self.b)\n",
        "      self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "      w_hat = local_kernel(self.u, self.v)\n",
        "    \n",
        "      sparse_reg = torch.nn.functional.mse_loss(w_hat, torch.zeros_like(w_hat))\n",
        "      sparse_reg_term = self.lambda_s * sparse_reg\n",
        "      \n",
        "      l2_reg = torch.nn.functional.mse_loss(self.W, torch.zeros_like(self.W))\n",
        "      l2_reg_term = self.lambda_2 * l2_reg\n",
        "\n",
        "      W_eff = self.W * w_hat  # Applying local kernelised weight matrix\n",
        "      y = torch.matmul(x, W_eff) + self.b\n",
        "      y = self.activation(y)\n",
        "\n",
        "      return y, sparse_reg_term + l2_reg_term\n",
        "\n",
        "# Class of a submodule of the final model that makes a pass through several kernel layers with diffrent arguments given(+ dropout = 0.33)\n",
        "class KernelNet(nn.Module):\n",
        "    def __init__(self, n_users, n_hidden, n_dim, n_layers, lambda_s, lambda_2):\n",
        "      super().__init__()\n",
        "      layers = []\n",
        "      for i in range(n_layers):\n",
        "        if i == 0:\n",
        "          layers.append(KernelLayer(n_users, n_hidden, n_dim, lambda_s, lambda_2))\n",
        "        else:\n",
        "          layers.append(KernelLayer(n_hidden, n_hidden, n_dim, lambda_s, lambda_2))\n",
        "      layers.append(KernelLayer(n_hidden, n_users, n_dim, lambda_s, lambda_2, activation=nn.Identity()))\n",
        "      self.layers = nn.ModuleList(layers)\n",
        "      self.dropout = nn.Dropout(0.33)\n",
        "\n",
        "    def forward(self, x):\n",
        "      total_reg = 0.0\n",
        "      for i, layer in enumerate(self.layers):\n",
        "        x, reg = layer(x)\n",
        "        \n",
        "        if (i < len(self.layers) - 1):\n",
        "          x = self.dropout(x)\n",
        "\n",
        "        total_reg += reg\n",
        "\n",
        "      return x, total_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Creating a class of the model that will be trained and after that used for recommending`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7RGKh1ckXgtP"
      },
      "outputs": [],
      "source": [
        "class CompleteNet(nn.Module):\n",
        "    def __init__(self, kernel_net, n_m, gk_size, dot_scale):\n",
        "      super().__init__()\n",
        "      self.gk_size = gk_size                # Global kernel size\n",
        "      self.dot_scale = dot_scale            # Scaling factitor for a convolution\n",
        "      self.local_kernel_net = kernel_net    # Pretrained \"local\" kernel net\n",
        "      self.conv_kernel = torch.nn.Parameter(torch.randn(n_m, gk_size**2) * 0.1)\n",
        "      nn.init.xavier_uniform_(self.conv_kernel, gain=torch.nn.init.calculate_gain(\"relu\"))\n",
        "      \n",
        "\n",
        "    # Applying the global kernel, performing a convolution, and applying the local kernel\n",
        "    def forward(self, x, x_local):\n",
        "      gk = self.global_kernel(x_local, self.gk_size, self.dot_scale)\n",
        "      x = self.global_conv(x, gk)\n",
        "      x, global_reg_loss = self.local_kernel_net(x)\n",
        "      return x, global_reg_loss\n",
        "\n",
        "    # Process of average pooling with applying the global kernel(+ scaled factor if != 1.0)\n",
        "    def global_kernel(self, input, gk_size, dot_scale):\n",
        "      avg_pooling = torch.mean(input, dim=1)\n",
        "      avg_pooling = avg_pooling.view(1, -1)\n",
        "\n",
        "      gk = torch.matmul(avg_pooling, self.conv_kernel) * dot_scale\n",
        "      gk = gk.view(1, 1, gk_size, gk_size)\n",
        "\n",
        "      return gk\n",
        "\n",
        "    # Performing a convolution...\n",
        "    def global_conv(self, input, W):\n",
        "      input = input.unsqueeze(0).unsqueeze(0)\n",
        "      conv2d = nn.LeakyReLU()(F.conv2d(input, W, stride=1, padding=1))\n",
        "      return conv2d.squeeze(0).squeeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Crating a custom loss class(MSE + L2 regularization)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Loss(nn.Module):\n",
        "    def forward(self, pred_p, reg_loss, train_m, train_r):\n",
        "      diff = train_m * (train_r - pred_p)\n",
        "      sqE = torch.nn.functional.mse_loss(diff, torch.zeros_like(diff))\n",
        "      loss_p = sqE + reg_loss\n",
        "      return loss_p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Creating functions for calculating a NDCG score`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dcg_k(score_label, k):\n",
        "    \"\"\"\n",
        "        Function that calculates DCG@K score\n",
        "    \"\"\"\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1] - 1) / np.log2(2 + i)\n",
        "            i += 1\n",
        "    return dcg\n",
        "\n",
        "def ndcg_k(y_hat, y, k):\n",
        "    \"\"\"\n",
        "        Function that calculates NDCG@K score\n",
        "    \"\"\"\n",
        "    score_label = torch.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1] - 1) / np.log2(2 + i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm\n",
        "\n",
        "def call_ndcg(y_hat, y):\n",
        "    \"\"\"\n",
        "        Function for calculating NDCG@K score for all the users\n",
        "    \"\"\"\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][torch.where(y[i])]\n",
        "        y_i = y[i][torch.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "### `Some hyperparameters for a traning process`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" Common hyperparameter settings: \"\"\"\n",
        "\n",
        "n_hidden = 500  # size of hidden layers\n",
        "n_dim = 5       # inner AutoEncoder embedding size\n",
        "n_layers = 2    # number of hidden layers\n",
        "gk_size = 3     # size of square kernel for convolution\n",
        "\n",
        "\n",
        "\"\"\" Hyperparameters to tune for specific case: \"\"\"\n",
        "\n",
        "max_epoch_p = 30    # max number of epochs for pretraining\n",
        "max_epoch_f = 1000  # max number of epochs for finetuning\n",
        "patience_p = 5      # number of consecutive rounds of early stopping condition before actual stop for pretraining\n",
        "patience_f = 10     # and finetuning\n",
        "tol_p = 1e-4        # minimum threshold for the difference between consecutive values of train rmse, used for early stopping, for pretraining\n",
        "tol_f = 1e-5        # and finetuning\n",
        "lambda_2 = 20.      # regularisation of number or parameters\n",
        "lambda_s = 0.006    # regularisation of sparsity of the final matrix\n",
        "dot_scale = 1       # dot product weight for global kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "### `\"Local kernel\" model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7teUrgWagpW0"
      },
      "outputs": [],
      "source": [
        "model = KernelNet(n_users, n_hidden, n_dim, n_layers, lambda_s, lambda_2).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "### `\"Global kernel\" model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "outputs": [],
      "source": [
        "complete_model = CompleteNet(model, n_movies, gk_size, dot_scale).double().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "### `Creating a function-step for an optimiser and prepare... prepared data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_r, test_r = torch.tensor(train_ratings, dtype=torch.double, device=device), torch.tensor(test_ratings, dtype=torch.float, device=device)\n",
        "train_m, test_m = torch.tensor(train_mask, dtype=torch.float, device=device), torch.tensor(test_mask, dtype=torch.float, device=device)\n",
        "\n",
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  complete_model.local_kernel_net.train()\n",
        "  pred, reg = complete_model.local_kernel_net(train_r)\n",
        "  loss = Loss().to(device)(pred, reg, train_m, train_r)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "optimizer = torch.optim.AdamW(complete_model.local_kernel_net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Running a pre-trainig loop`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ35Zoha-Eue",
        "outputId": "4fc0c647-b0a5-4e69-afff-899c14dc247d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 27/30 [00:03<00:00,  7.93it/s, Train RMSE:=1.12, Test RMSE:=1.13]\n"
          ]
        }
      ],
      "source": [
        "# Creating some variables for storing RMSEs, early-stop counts, and last output of the \"local kernel\" net\n",
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = np.inf, np.inf, 0\n",
        "\n",
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "train_r_local = None\n",
        "\n",
        "bar = tqdm(range(max_epoch_p))\n",
        "\n",
        "for i in bar:\n",
        "  optimizer.step(closure)                   # Traing \"local kernel\" net\n",
        "  complete_model.local_kernel_net.eval()    # And switch the net to an eval mode\n",
        "\n",
        "  pre, _ = model(train_r)                   # Perform a \"whole model\" prediction\n",
        "  train_r_local = torch.clip(pre, 1., 5.)\n",
        "\n",
        "  pre = pre.float()\n",
        "  \n",
        "  error = (test_m * (torch.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()           # test error\n",
        "  test_rmse = np.sqrt(error.item())\n",
        "\n",
        "  error_train = (train_m * (torch.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train.item())\n",
        "\n",
        "  # Some sort of an early-stopping\n",
        "  if last_rmse - train_rmse < tol_p:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  if patience_p == counter:\n",
        "    break\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "  bar.set_postfix({\"Train RMSE:\": train_rmse, \"Test RMSE:\": test_rmse})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Same steps as for a \"pre-training\" step(+ passing a last complemented matrix by the \"local kernel\" net)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def closure():\n",
        "  optimizer.zero_grad()\n",
        "  x_local = train_r_local.clone().detach()\n",
        "  complete_model.train()\n",
        "  pred, reg = complete_model(train_r, x_local)\n",
        "  loss = Loss().to(device)(pred, reg, train_m, train_r)\n",
        "  loss.backward()\n",
        "  return loss\n",
        "\n",
        "optimizer = torch.optim.AdamW(complete_model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Running a fine-tuning loop in the same manner as the pre-training one(+ calculating NDCG@K score)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6v_tODcweLn",
        "outputId": "5ab3f058-5c7c-458a-c3b5-32bbcf4df418"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [26:49<00:00,  1.61s/it, test rmse:=0.896, test mae:=0.705, test ndcg:=0.907, train rmse:=0.828, train mae:=0.655, train ndcg:=0.914]\n"
          ]
        }
      ],
      "source": [
        "last_rmse = np.inf\n",
        "counter = 0\n",
        "# test_RMSE_losses, test_NDCG_losses = [], []\n",
        "# train_RMSE_losses, train_NDCG_losses = [], []\n",
        "\n",
        "bar = tqdm(range(max_epoch_f))\n",
        "\n",
        "for i in bar:\n",
        "  optimizer.step(closure)\n",
        "  complete_model.eval()\n",
        "\n",
        "  pre, _ = complete_model(train_r, train_r_local)\n",
        "  \n",
        "  pre = pre.float()\n",
        "\n",
        "  error = (test_m * (torch.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()           # test error\n",
        "  test_rmse = np.sqrt(error.item())\n",
        "\n",
        "  error_train = (train_m * (torch.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "  train_rmse = np.sqrt(error_train.item())\n",
        "\n",
        "  # train_RMSE_losses.append(train_rmse)\n",
        "  # test_RMSE_losses.append(test_rmse)\n",
        "\n",
        "  # Questionable content(but it presents)\n",
        "  test_mae = ((test_m * torch.abs(torch.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()).item()       \n",
        "  train_mae = ((train_m * torch.abs(torch.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()).item()\n",
        "\n",
        "  test_ndcg = call_ndcg(torch.clip(pre, 1., 5.), test_r)\n",
        "  train_ndcg = call_ndcg(torch.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "  # train_NDCG_losses.append(train_ndcg)\n",
        "  # test_NDCG_losses.append(test_ndcg)\n",
        "\n",
        "  # Questionable content... again(but for statistics is OK)\n",
        "  if test_rmse < best_rmse:\n",
        "      best_rmse = test_rmse\n",
        "      best_rmse_ep = i + 1\n",
        "\n",
        "  if test_mae < best_mae:\n",
        "      best_mae = test_mae\n",
        "      best_mae_ep = i + 1\n",
        "\n",
        "  if best_ndcg < test_ndcg:\n",
        "      best_ndcg = test_ndcg\n",
        "      best_ndcg_ep = i + 1\n",
        "\n",
        "  # Early-stopping check\n",
        "  if last_rmse - train_rmse < tol_f:\n",
        "    counter += 1\n",
        "  else:\n",
        "    counter = 0\n",
        "\n",
        "  if patience_f == counter:\n",
        "    break\n",
        "\n",
        "  last_rmse = train_rmse\n",
        "\n",
        "  bar.set_postfix({'test rmse:': test_rmse, 'test mae:': test_mae, 'test ndcg:': test_ndcg, \n",
        "                   'train rmse:': train_rmse, 'train mae:': train_mae, 'train ndcg:': train_ndcg}\n",
        "  )\n",
        "\n",
        "torch.save(model.state_dict(), \"../models/best_local_kernel.pt\")\n",
        "torch.save(complete_model.state_dict(), \"../models/best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Printing results: best RMSE, best MSE, best NDCG@K and epoch number where their where acquired`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTi_PdXJqTjh",
        "outputId": "6f5a2d63-2c1f-446f-8f20-d4357d4bdc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 839  best rmse: 0.8950777008915999\n",
            "Epoch: 961  best mae: 0.7030419707298279\n",
            "Epoch: 248  best ndcg: 0.9085106419580863\n"
          ]
        }
      ],
      "source": [
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `Taking a top K recommendations for a curtain user`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_local_kernel = KernelNet(n_users, n_hidden, n_dim, n_layers, lambda_s, lambda_2).double().to(device)\n",
        "test_complete_model = CompleteNet(test_local_kernel, n_movies, gk_size, dot_scale).double().to(device)\n",
        "\n",
        "test_local_kernel.load_state_dict(torch.load(\"../models/best_local_kernel.pt\"))\n",
        "test_complete_model.load_state_dict(torch.load(\"../models/best.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "b6Yfh3hm4Efa"
      },
      "outputs": [],
      "source": [
        "test_complete_model.eval()\n",
        "pre, _ = test_complete_model(train_r, train_r_local)\n",
        "pre = pre.cpu().detach().numpy()\n",
        "\n",
        "user_id = 7         # Considering the fact that it is actual user_id - 1\n",
        "K = 10              # If K == train_rating.shape[1] then shows all the movies(for which there is no rating) in the order of decreasing predicted rating\n",
        "\n",
        "pre *= (np.ones_like(train_mask) - train_mask)  # Reversed mask for taking the movies for which there is no rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[178 357 515 603 483 427 480  98 318  64]\n"
          ]
        }
      ],
      "source": [
        "pre = (pre * (pre > 3.5))[:, user_id]   # Taking into account only movies with predicted rating bigger than 3.5\n",
        "recs = np.argsort(pre)[-K:]             # Taking top K movie indices\n",
        "print(recs + 1)                         # Showing movie IDs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
